# -*- coding: utf-8 -*-
"""course.cse.ust.hk_comp2211_assignments_pa1_pa1_task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VkwatDGVKEJ0svFbNKIG2Vxck2nY8SqN
"""

import numpy as np
import pandas as pd

if __name__ == '__main__':
    from google.colab import drive
    drive.mount('/content/drive')
    data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/PA1/winequality-white.csv', delimiter=';')

if __name__ == '__main__':
    [numRow, numCol] = data.shape
    print("data shape:", data.shape)
    data.head()

#Splitting data into testing and training samples
if __name__ == '__main__':

    traindata=data.iloc[:4000]
    testdata=data.iloc[4000:]

    X_train=traindata.drop(columns=["quality"], axis=1).to_numpy()
    y_train=traindata.iloc[:,11].to_numpy()

    X_test=testdata.drop(columns=["quality"], axis=1).to_numpy()
    y_test=testdata.iloc[:,11].to_numpy()



    print("X_train shape: {} and y_train shape: {}".format(X_train.shape, y_train.shape))
    print("X_test shape: {} and y_test shape: {}".format(X_test.shape, y_test.shape))

# Min Max Normalization
def min_max_normalization(input_array):

  min=np.min(input_array,axis=0)
  max=np.max(input_array,axis=0)
  normalized_array= (input_array-min)/(max-min)


  return normalized_array

# Z Score  Normalization
def z_score_normalization(input_array):
  mean=np.mean(input_array,axis=0)
  std=np.std(input_array,axis=0)
  normalized_array= (input_array-mean)/std



  return normalized_array

# Euclidean Distance Calculation
def euclidean_distance(X_train, X_test):

  temptest=np.expand_dims(X_test,axis=1)
  distance= np.sqrt(np.sum(((X_train-temptest)**2),2))
  return distance

# Manhattan Distance Calculation
def manhattan_distance(X_train, X_test):

  temptest=np.expand_dims(X_test,axis=1)
  distance= (np.sum(np.abs(X_train-temptest),2))


  return distance

#Finding the knearest neighbors
def find_k_nearest_neighbor(distance, y_train, k):

  sorted=np.argsort(distance)
  sortedwithvalues=np.take_along_axis(distance, sorted, axis=1)
  distance_neighbor=np.take(sortedwithvalues,np.arange(k),axis=1)
  y_neighbor=np.take(np.take_along_axis(np.expand_dims(y_train,1),sorted,axis=0),np.arange(k),axis=1)


  return y_neighbor, distance_neighbor
  # y_neighbor: numpy array of shape (num_rows_test, k), the labels of the k nearest neighbors of each test point
  # distance_neighbor: numpy array of shape (num_rows_test, k),  the distance between each test point and its k nearest neighbors

#Implementing a weighted average prediction
def weighted_average_predict(y_neighbor, weights=None):
  # y_neighbor: numpy array of shape (num_rows_test, k), the labels of the k nearest neighbors of each test point
  # weights: numpy array of shape (k, ), controls the contribution of each near enighbor
  if weights is None:
    weights=np.ones(y_neighbor.shape[1])
  weightsum=np.sum(weights)
  prediction=np.dot(y_neighbor,weights)/weightsum

  return prediction
  temp=np.expand_dims(weights,axis=1)
  sumprod=np.matmul(y_neighbor, temp)



  prediction=np.squeeze(sumprod/weightsum)
  print(prediction)

  return prediction

#Implementing a distance based prediction

def distance_based_predict(y_neighbor, distance_neighbor, epsilon):
  # y_neighbor: numpy array of shape (num_rows_test, k), the labels of the k nearest neighbors of each test point
  # distance_neighbor, numpy array of shape (num_rows_test, k), the distance between the each k nearest neighbor and test point
  # epsilon: positive number, to avoid dividing by zero problem
  weights=1/(distance_neighbor+epsilon)
  weightsum=np.sum(weights,axis=1)
  product=y_neighbor*weights
  sumprod=np.sum(product,axis=1)
  prediction=sumprod/weightsum

  return prediction

#Implementing error calculators: MAE,MSE, MAPE
def metric_analyze(y_true, y_pred):
  difference=y_true-y_pred
  absdifference1=np.absolute(difference)
  absdifference2=np.absolute(difference/y_true)
  differencesquared=(difference**2)
  n=y_true.shape
  mae=(np.sum(absdifference1))/n
  mse=(np.sum(difference**2))/n
  mape=(np.sum(absdifference2))/n



  return (mae[0], mse[0], mape[0])

#Splitting into D-folds
def split_d_fold(X, y, d):
  # X: numpy array of shape (num_rows, num_features), the feature data
  # y: numpy array of shape (num_rows, ), the label data
  # d: integer, number of folds
  data = np.concatenate((X, y[:, np.newaxis]), axis=1) # for better data structure
  temp=np.array_split(data,d)
  train_d_folds=[]
  test_d_folds=[]

  for fold in range(d):
    test_d_folds.append(temp[fold])

    traintemp= np.array_split(data,d)
    del traintemp[fold]
    tempp=np.concatenate(traintemp[:d])
    train_d_folds.append(tempp)

  return train_d_folds, test_d_folds

#Cross validation using D-fold
def cross_validate(train_d_folds, test_d_folds, k_list):
  # train_d_folds: a pyhon list of length d, each entry is a training fold (numpy array)
  # test_d_folds: a python list of length d, each entry is a test fold (numpy array)
  # k_list: a python list, contains the k(s) to be validated
  # the i-th train_fold and test_fold are corresponding
  scores = np.zeros((len(k_list), len(train_d_folds)))
  # scores: a numpy array of shape (len(k_list), len(d_folds)), contains the metric for specific k and fold
  for k_index, k in enumerate(k_list):
    for fold in range(len(train_d_folds)):

      a=(np.array(train_d_folds)[fold])
      b=np.array(test_d_folds[fold])

      X_train=a[:,:(np.shape(a)[1]-1)]
      y_train=a[:,-1]

      X_test=b[:,:(np.shape(a)[1]-1)]
      y_test=b[:,-1]




      distance= euclidean_distance(X_train,X_test)
      (y_neighbor, distance_neighbor)=find_k_nearest_neighbor(distance,y_train,k)
      prediction=weighted_average_predict(y_neighbor)
      (mae,mse,mape)=metric_analyze(y_test,prediction)
      place=(k_index*len(train_d_folds))+(fold)
      np.put(scores,place,mse)

  mean_scores = np.mean(scores, axis=1, keepdims=False)
  return mean_scores

if __name__ == '__main__':
    normalized_X_train = min_max_normalization(X_train)
    normalized_X_test = min_max_normalization(X_test)
    train_d_folds, test_d_folds = split_d_fold(normalized_X_train, y_train, 5)
    print("cross-validation scores: \n", cross_validate(train_d_folds, test_d_folds, [11,15,19]))

import tempfile
#Finding the best k
def find_best_k(k_list, mean_scores):
  # k_list: a python list, contains the k to be validated, order is not guaranteed
  # mean_scores: a numpy array of shape (len(k_list), ), the mean score for each k

  temp=np.array(k_list)


  sorted=np.argsort(temp)

  sortedwithvalues=np.take_along_axis(temp, sorted,axis=0)

  sortedmeans=np.take(mean_scores,sorted)


  min=np.argmin(sortedmeans)

  best_k=sortedwithvalues[min]


  return best_k